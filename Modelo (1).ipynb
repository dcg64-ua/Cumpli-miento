{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaAXRslVRqWQ"
      },
      "source": [
        "# **Cumpli-Miento: Detección de cumplimiento del RGPD en políticas de privacidad web**\n",
        "\n",
        "- Autores: Daniel Carreres y Juan Sanz\n",
        "- Fecha: 27/07/2024\n",
        "- Versión: 1.0\n",
        "\n",
        "# Tabla de contenidos (índice)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Cumpli-Miento: Detección de cumplimiento del RGPD en políticas de privacidad web](#scrollTo=xaAXRslVRqWQ)\n",
        "\n",
        ">[Tabla de contenidos (índice)](#scrollTo=xaAXRslVRqWQ)\n",
        "\n",
        ">>[0. Cargar los datos](#scrollTo=qgwDlksWG55_)\n",
        "\n",
        ">>[1. Preparación de los Datos](#scrollTo=EyJFhIcuHpab)\n",
        "\n",
        ">>[2. Definición y Entrenamiento de Modelos](#scrollTo=U-OvYGEiG9kE)\n",
        "\n",
        ">>[3. Evaluación de Políticas de Privacidad](#scrollTo=mLk5ZVbTH6qW)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "Itfnt1aBIMgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Cargar los datos"
      ],
      "metadata": {
        "id": "qgwDlksWG55_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Definir la ruta del archivo zip cargado\n",
        "zip_file_path = '/content/privacy_policys (4).zip'  # Cambia esta ruta por la ruta correcta de tu archivo zip\n",
        "extract_path = '/content/privacy_policys'  # Cambia esta ruta por la ruta donde quieres descomprimir los archivos\n",
        "\n",
        "# Descomprimir el archivo zip\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Listar los archivos extraídos para asegurarse de que se han descomprimido correctamente\n",
        "extracted_files = os.listdir(extract_path)\n",
        "print(\"Archivos extraídos:\", extracted_files[:10])  # Muestra solo los primeros 10 archivos para brevedad\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJYSaRt0kxMu",
        "outputId": "16268237-8b48-4a4c-fa65-a4a53d2d0bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivos extraídos: ['privacy_policys (2)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preparación de los Datos"
      ],
      "metadata": {
        "id": "EyJFhIcuHpab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar el archivo CSV\n",
        "csv_file_path = '/content/Refined_Corrected_Data.csv'\n",
        "data = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Verificar los archivos en el directorio\n",
        "extract_path = '/content/privacy_policys/privacy_policys (2)'\n",
        "renamed_files = os.listdir(extract_path)\n",
        "print(\"Archivos renombrados:\", renamed_files[:10])\n",
        "\n",
        "# Añadir una nueva columna 'Filename' en el DataFrame con los nombres simples de las empresas\n",
        "data['Filename'] = data['Empresa'].apply(lambda x: x + '.html')\n",
        "\n",
        "# Verificar las primeras filas del DataFrame\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "qwh8H2dioQHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Definición y Entrenamiento de Modelos"
      ],
      "metadata": {
        "id": "U-OvYGEiG9kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la función para extraer texto de archivos HTML\n",
        "def extract_text_from_html(file_path):\n",
        "    if file_path and os.path.isfile(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "        return text\n",
        "    return None\n",
        "\n",
        "# Aplicar la función refinada al DataFrame\n",
        "def match_and_extract_text_using_filenames(df, extract_path):\n",
        "    df['Texto'] = df['Filename'].apply(lambda filename: extract_text_from_html(os.path.join(extract_path, filename)))\n",
        "    return df\n",
        "\n",
        "# Aplicar la función para extraer texto al DataFrame\n",
        "data_with_text = match_and_extract_text_using_filenames(data, extract_path)\n",
        "\n",
        "# Filtrar las filas donde el texto fue extraído correctamente\n",
        "data_with_text = data_with_text.dropna(subset=['Texto'])\n",
        "\n",
        "# Mostrar el DataFrame con el texto incluido\n",
        "print(data_with_text.head())\n"
      ],
      "metadata": {
        "id": "YgFxh6zsoeHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar las columnas del DataFrame\n",
        "print(\"Columnas del DataFrame:\", data.columns)\n",
        "\n",
        "# Añadir nuevamente la columna 'Filename' en el DataFrame\n",
        "data['Filename'] = data['Empresa'].apply(lambda x: x.lower() + '.html')\n",
        "\n",
        "# Verificar las primeras filas del DataFrame con la nueva columna\n",
        "print(data.head())\n",
        "\n",
        "# Aplicar la función refinada al DataFrame para extraer texto\n",
        "def match_and_extract_text_using_filenames(df, extract_path):\n",
        "    df['Texto'] = df['Filename'].apply(lambda filename: extract_text_from_html(os.path.join(extract_path, filename)))\n",
        "    return df\n",
        "\n",
        "# Aplicar la función para extraer texto al DataFrame\n",
        "data_with_text = match_and_extract_text_using_filenames(data, extract_path)\n",
        "\n",
        "# Filtrar las filas donde el texto fue extraído correctamente\n",
        "data_with_text = data_with_text.dropna(subset=['Texto'])\n",
        "\n",
        "# Mostrar el DataFrame con el texto incluido\n",
        "data_with_text.head()\n"
      ],
      "metadata": {
        "id": "vXdV37W8p5Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluación de Políticas de Privacidad"
      ],
      "metadata": {
        "id": "mLk5ZVbTH6qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Función para extraer texto de una URL\n",
        "def extract_text_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return text\n",
        "    else:\n",
        "        print(f\"Error al acceder a la URL: {response.status_code}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "k7GnCL93q3X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Derechos a evaluar\n",
        "derechos = ['Derecho a ser informado', 'Derecho de acceso', 'Derecho de rectificación',\n",
        "            'Derecho de eliminación', 'Derecho de procesamiento restringido',\n",
        "            'Derecho a la portabilidad de los datos', 'Derecho de oposición',\n",
        "            'Derechos relacionados con la toma de decisiones y la elaboración de perfiles automatizados']\n",
        "\n",
        "# Diccionario para almacenar modelos y vectorizadores para cada derecho\n",
        "models = {}\n",
        "vectorizers = {}\n",
        "\n",
        "# Entrenar un modelo para cada derecho\n",
        "for derecho in derechos:\n",
        "    X = data_with_text['Texto']\n",
        "    y = data_with_text[derecho]\n",
        "\n",
        "    # Verificar que haya al menos dos clases en los datos\n",
        "    if len(y.unique()) > 1:\n",
        "        # Crear un pipeline de TF-IDF Vectorizer y Logistic Regression\n",
        "        pipeline = make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=1000))\n",
        "\n",
        "        # Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Entrenar el modelo\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluar el modelo\n",
        "        accuracy = pipeline.score(X_test, y_test)\n",
        "        print(f\"Accuracy for {derecho}: {accuracy}\")\n",
        "\n",
        "        # Guardar el modelo y el vectorizador\n",
        "        models[derecho] = pipeline.named_steps['logisticregression']\n",
        "        vectorizers[derecho] = pipeline.named_steps['tfidfvectorizer']\n",
        "    else:\n",
        "        print(f\"No hay suficientes datos para entrenar el modelo para {derecho}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q99OjYa1rYf8",
        "outputId": "0009b3f1-3628-458e-c495-6b59ba8413c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No hay suficientes datos para entrenar el modelo para Derecho a ser informado\n",
            "Accuracy for Derecho de acceso: 1.0\n",
            "Accuracy for Derecho de rectificación: 1.0\n",
            "Accuracy for Derecho de eliminación: 1.0\n",
            "Accuracy for Derecho de procesamiento restringido: 0.7777777777777778\n",
            "Accuracy for Derecho a la portabilidad de los datos: 1.0\n",
            "Accuracy for Derecho de oposición: 1.0\n",
            "Accuracy for Derechos relacionados con la toma de decisiones y la elaboración de perfiles automatizados: 0.5555555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Función para extraer texto de una URL\n",
        "def extract_text_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return text\n",
        "    else:\n",
        "        print(f\"Error al acceder a la URL: {response.status_code}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "4VEjPI7WrsOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para evaluar el texto extraído para cada derecho\n",
        "def evaluate_rights_from_url(url, models, vectorizers):\n",
        "    text = extract_text_from_url(url)\n",
        "    if text:\n",
        "        results = {}\n",
        "        for derecho in derechos:\n",
        "            if derecho in models:\n",
        "                vectorizer = vectorizers[derecho]\n",
        "                model = models[derecho]\n",
        "                X_new = vectorizer.transform([text])\n",
        "                prediction = model.predict(X_new)\n",
        "                results[derecho] = 'Cumple' if prediction[0] else 'No cumple'\n",
        "            else:\n",
        "                results[derecho] = 'Sin datos suficientes'\n",
        "        return results\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Ejemplo de URL para evaluar\n",
        "url_to_evaluate = \"https://pokedoku.com/policy\"\n",
        "results = evaluate_rights_from_url(url_to_evaluate, models, vectorizers)\n",
        "print(\"Resultados de la evaluación:\")\n",
        "for derecho, resultado in results.items():\n",
        "    print(f\"{derecho}: {resultado}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yglzEI84ruGk",
        "outputId": "e13dd310-26d3-4a82-8dc8-d8ead3c2c0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados de la evaluación:\n",
            "Derecho a ser informado: Sin datos suficientes\n",
            "Derecho de acceso: Cumple\n",
            "Derecho de rectificación: Cumple\n",
            "Derecho de eliminación: Cumple\n",
            "Derecho de procesamiento restringido: Cumple\n",
            "Derecho a la portabilidad de los datos: Cumple\n",
            "Derecho de oposición: Cumple\n",
            "Derechos relacionados con la toma de decisiones y la elaboración de perfiles automatizados: Cumple\n"
          ]
        }
      ]
    }
  ]
}